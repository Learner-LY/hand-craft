{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#利用pytorch来进行优化;\n",
    "import numpy as np\n",
    "import h5py\n",
    "from skimage import io\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#优化实验!\n",
    "x = Variable(torch.ones(1), requires_grad=True)\n",
    "y = Variable(torch.ones(1), requires_grad=True)\n",
    "z=2*x**2+3*y**3\n",
    "z.backward(retain_variables=True)\n",
    "learning_rate = 0.1\n",
    "x.data.sub_(learning_rate*x.grad.data)\n",
    "x.grad.data=torch.FloatTensor([0.0])\n",
    "z.backward(retain_variables=True)\n",
    "#w1.data -= learning_rate * w1.grad.data\n",
    "x.data.sub_(learning_rate*x.grad.data)\n",
    "#手动更新也可行;\n",
    "\n",
    "x = Variable(torch.ones(1), requires_grad=True)\n",
    "y = Variable(torch.ones(1), requires_grad=True)\n",
    "z=2*x**2+3*y**3\n",
    "#使用pytorch中的optim优化器;\n",
    "optimizer = optim.SGD([x,y], lr = 0.01)\n",
    "optimizer.zero_grad() # zero the gradient buffers，必须要置零\n",
    "z.backward(retain_variables=True)\n",
    "optimizer.step() # Does the update\n",
    "#可行！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#variable:s_mean;alpha;s;S;f;P;R;t;U_proj;d;U_label;sigma;\n",
    "f=h5py.File('shapemodel.h5','r')\n",
    "fly=open('testface_landmarks.txt')\n",
    "ftxt=fly.readlines()\n",
    "for i in range(len(ftxt)):\n",
    "    ftxt[i]=ftxt[i].strip('\\n').split(' ')\n",
    "for i in range(68):\n",
    "    for j in range(2):\n",
    "        ftxt[i][j]=float(ftxt[i][j])\n",
    "U_label=Variable(torch.FloatTensor(np.array(ftxt).T))\n",
    "\n",
    "d=f['keypoints'][:][0]\n",
    "s_mean=Variable(torch.FloatTensor(f['mean_shape'][:]))   #159645*1;\n",
    "s=Variable(torch.FloatTensor(f['pca_basis'][:]))    #159645*50;\n",
    "P=Variable(torch.FloatTensor(np.array([[1,0,0],[0,1,0]])))\n",
    "sigma=Variable(torch.FloatTensor(f['sigma'][:]))   #50*1;\n",
    "\n",
    "alpha=Variable(torch.rand((50,1)),requires_grad=True)\n",
    "S=s_mean+s.mm(alpha)\n",
    "S=S.view(53215,3)\n",
    "f=Variable(torch.ones((2,3)),requires_grad=True)\n",
    "R=Variable(torch.rand((3,3)),requires_grad=True)\n",
    "t=Variable(torch.rand(2),requires_grad=True)\n",
    "S_final=Variable(torch.rand(3,68))\n",
    "for i in range(68):\n",
    "    S_final[:,i]=S[d[i]]\n",
    "U_proj=(f*P).mm(R).mm(S_final)\n",
    "for i in range(68):\n",
    "    U_proj[:,i]=U_proj[:,i]+t\n",
    "loss2=torch.sum((alpha*alpha)/(sigma*sigma))\n",
    "loss1=torch.sqrt(torch.sum((U_proj-U_label)*(U_proj-U_label)))\n",
    "loss=loss1+loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate=0.0005\n",
    "loss_100=0\n",
    "loss_print=[]\n",
    "loss_new=0\n",
    "for i in range(10000):\n",
    "    #loss_old=torch.sum((alpha*alpha)/(sigma*sigma))+torch.sqrt(torch.sum((U_proj-U_label)*(U_proj-U_label)))\n",
    "    f.grad.data=torch.FloatTensor([[0.0,0.0,0.0],[0.0,0.0,0.0]])\n",
    "    R.grad.data=torch.zeros((3,3))\n",
    "    t.grad.data=torch.zeros((2,1))\n",
    "    alpha.grad.data=torch.zeros((50,1))\n",
    "    loss.backward(retain_variables=True)\n",
    "    #w1.data -= learning_rate * w1.grad.data\n",
    "    f.data.sub_(learning_rate*f.grad.data)\n",
    "    R.data.sub_(learning_rate*R.grad.data)\n",
    "    t.data.sub_(learning_rate*t.grad.data)\n",
    "    alpha.data.sub_(learning_rate*alpha.grad.data)    \n",
    "    loss_new=torch.sum((alpha*alpha)/(sigma*sigma))+torch.sqrt(torch.sum((U_proj-U_label)*(U_proj-U_label)))\n",
    "    loss_100=loss_100+loss_new\n",
    "    if i%100==0:\n",
    "        loss_print.append(loss_100/100.0)\n",
    "        loss_100=0\n",
    "    if i%1000==0:\n",
    "        print i\n",
    "\n",
    "stop=1\n",
    "while stop>0:\n",
    "    #loss_old=torch.sum((alpha*alpha)/(sigma*sigma))+torch.sqrt(torch.sum((U_proj-U_label)*(U_proj-U_label)))\n",
    "    f.grad.data=torch.FloatTensor([[0.0,0.0,0.0],[0.0,0.0,0.0]])\n",
    "    R.grad.data=torch.zeros((3,3))\n",
    "    t.grad.data=torch.zeros((2,1))\n",
    "    alpha.grad.data=torch.zeros((50,1))\n",
    "    loss.backward(retain_variables=True)\n",
    "    #w1.data -= learning_rate * w1.grad.data\n",
    "    f.data.sub_(learning_rate*f.grad.data)\n",
    "    R.data.sub_(learning_rate*R.grad.data)\n",
    "    t.data.sub_(learning_rate*t.grad.data)\n",
    "    alpha.data.sub_(learning_rate*alpha.grad.data)\n",
    "    loss_new=torch.sum((alpha*alpha)/(sigma*sigma))+torch.sqrt(torch.sum((U_proj-U_label)*(U_proj-U_label)))\n",
    "    loss_100=loss_100+loss_new.data\n",
    "    if i%100==0:\n",
    "        loss_print.append(loss_100/100.0)\n",
    "        loss_100=0\n",
    "        if len(loss_print)>2 and torch.abs(loss_print[len(loss_print)-1]-loss_print[len(loss_print)-2])<0.1:\n",
    "            stop=0\n",
    "    if i%1000==0:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([f,R,t,alpha], lr = 0.001)\n",
    "loss_100=0\n",
    "loss_print=[]\n",
    "loss_new=0\n",
    "stop=1\n",
    "while stop>0:\n",
    "    #loss_old=torch.sum((alpha*alpha)/(sigma*sigma))+torch.sqrt(torch.sum((U_proj-U_label)*(U_proj-U_label)))\n",
    "    optimizer.zero_grad() # zero the gradient buffers，必须要置零\n",
    "    loss.backward(retain_variables=True)\n",
    "    optimizer.step() # Does the update\n",
    "    loss_new=torch.sum((alpha*alpha)/(sigma*sigma))+torch.sqrt(torch.sum((U_proj-U_label)*(U_proj-U_label)))\n",
    "    loss_100=loss_100+loss_new.data\n",
    "    if i%100==0:\n",
    "        loss_print.append(loss_100/100.0)\n",
    "        loss_100=0\n",
    "        if len(loss_print)>2 and torch.abs(loss_print[len(loss_print)-1]-loss_print[len(loss_print)-2])<0.1:\n",
    "            stop=0\n",
    "    if i%1000==0:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.228512 0.414567 0.452815 0.484734 0.092824 0.238131 0.947343 0.195758 0.773634 0.910061 0.418036 0.186317 0.224193 0.967962 0.364416 0.705933 0.078815 0.705428 0.483994 0.168831 0.370696 0.721676 0.638648 0.039472 0.778362 0.893754 0.354865 0.246894 0.216938 0.894282 0.563633 0.351977 0.099967 0.660270 0.582316 0.510889 0.898550 0.362596 0.986510 0.959051 0.743724 0.804989 0.059532 0.871163 0.170256 0.035645 0.679062 0.528763 0.242468 0.172112 "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "for i in range(50):\n",
    "    print('%f' % alpha.data.numpy()[i],end=' ')\n",
    "    #print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#提交时对格式有要求:注意print出来的格式，有时候看着在一行但是实际有换行符，就会提交不了;\n",
    "#对提交的结果格式解析时没有强制要求alpha的和为1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6445.176875\n",
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph second time, but the buffers have already been freed. Please specify retain_variables=True when calling backward for the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-870a0a89f435>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m#loss_old=torch.sum((alpha*alpha)/(sigma*sigma))+torch.sqrt(torch.sum((U_proj-U_label)*(U_proj-U_label)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# zero the gradient buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Does the update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mloss_new\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU_proj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mU_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU_proj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mU_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ly/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ly/anaconda2/lib/python2.7/site-packages/torch/autograd/_functions/pointwise.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_output)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph second time, but the buffers have already been freed. Please specify retain_variables=True when calling backward for the first time."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from skimage import io\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "#variable:s_mean;alpha;s;S;f;P;R;t;U_proj;d;U_label;sigma;\n",
    "f=h5py.File('shapemodel.h5','r')\n",
    "fly=open('testface_landmarks.txt')\n",
    "ftxt=fly.readlines()\n",
    "for i in range(len(ftxt)):\n",
    "    ftxt[i]=ftxt[i].strip('\\n').split(' ')\n",
    "for i in range(68):\n",
    "    for j in range(2):\n",
    "        ftxt[i][j]=float(ftxt[i][j])\n",
    "U_label=Variable(torch.FloatTensor(np.array(ftxt).T))\n",
    "\n",
    "d=f['keypoints'][:][0]\n",
    "s_mean=Variable(torch.FloatTensor(f['mean_shape'][:]))   #159645*1;\n",
    "s=Variable(torch.FloatTensor(f['pca_basis'][:]))    #159645*50;\n",
    "P=Variable(torch.FloatTensor(np.array([[1,0,0],[0,1,0]])))\n",
    "sigma=Variable(torch.FloatTensor(f['sigma'][:]))   #50*1;\n",
    "\n",
    "alpha=Variable(torch.rand((50,1)),requires_grad=True)\n",
    "S=s_mean+s.mm(alpha)\n",
    "S=S.view(53215,3)\n",
    "f=Variable(torch.ones((2,3)),requires_grad=True)\n",
    "R=Variable(torch.rand((3,3)),requires_grad=True)\n",
    "t=Variable(torch.rand((2)),requires_grad=True)\n",
    "S_final=Variable(torch.rand(3,68))\n",
    "for i in range(68):\n",
    "    S_final[:,i]=S[d[i]]\n",
    "U_proj=(f*P).mm(R).mm(S_final)\n",
    "for i in range(68):\n",
    "    U_proj[:,i]=U_proj[:,i]+t\n",
    "loss2=torch.sum((alpha*alpha)/(sigma*sigma))\n",
    "loss1=torch.sqrt(torch.sum((U_proj-U_label)*(U_proj-U_label)))\n",
    "loss=loss1+loss2\n",
    "\n",
    "optimizer = optim.SGD([f,R,t,alpha], lr = 0.0001)\n",
    "loss_100=0\n",
    "loss_print=[]\n",
    "loss_new=0\n",
    "#stop=1\n",
    "#while stop>0:\n",
    "for i in range(1000):\n",
    "    #loss_old=torch.sum((alpha*alpha)/(sigma*sigma))+torch.sqrt(torch.sum((U_proj-U_label)*(U_proj-U_label)))\n",
    "    optimizer.zero_grad() # zero the gradient buffers\n",
    "    loss.backward(retain_variables=True)\n",
    "    optimizer.step() # Does the update\n",
    "    loss_new=torch.sum((alpha*alpha)/(sigma*sigma))+torch.sqrt(torch.sum((U_proj-U_label)*(U_proj-U_label)))\n",
    "    loss_100=loss_100+loss_new.data.numpy()[0]\n",
    "    if i%10==0:\n",
    "        print loss_100/100.0\n",
    "        loss_print.append(loss_100/100.0)\n",
    "        loss_100=0\n",
    "        #if len(loss_print)>2 and torch.abs(loss_print[len(loss_print)-1]-loss_print[len(loss_print)-2])<0.1:\n",
    "           # stop=0\n",
    "    if i%10==0:\n",
    "        print i\n",
    "#f_out=f.data.view(6).numpy()\n",
    "#R_out=R.data.view(9).numpy()\n",
    "#t_out=t.data.view(2).numpy()\n",
    "#alpha_out=alpha.data.view(50).numpy()\n",
    "#np.savetxt('f.txt',f_out,fmt='%.4f')\n",
    "#np.savetxt('R.txt',R_out,fmt='%.4f')\n",
    "#np.savetxt('t.txt',t_out,fmt='%.4f')\n",
    "#np.savetxt('alpha.txt',alpha_out,fmt='%.4f')\n",
    "#np.savetxt('loss.txt',loss_print,fmt='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=Variable(torch.ones((2,3)),requires_grad=True)\n",
    "ff=torch.ones(2,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fmt控制输出格式和列之间的分隔符，其中的设置次数要和列数对应；newline对应不同行之间的分隔符;\n",
    "#大小不同的数组好像不能同时读入，也没有办法不覆盖在文件末尾追加!\n",
    "np.savetxt('1.txt',(ly,sr),fmt='%.4f %.4f %.4f %.4f',newline=' ')\n",
    "ly=np.array([1,2])\n",
    "sr=np.array([[5,6,7,8]])\n",
    "np.savetxt('3.txt',(ly,sr),fmt='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the part for try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from skimage import io\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#variable:s_mean;alpha;s;S;f;P;R;t;U_proj;d;U_label;sigma;\n",
    "f=h5py.File('shapemodel.h5','r')\n",
    "fly=open('testface_landmarks.txt')\n",
    "ftxt=fly.readlines()\n",
    "for i in range(len(ftxt)):\n",
    "    ftxt[i]=ftxt[i].strip('\\n').split(' ')\n",
    "for i in range(68):\n",
    "    for j in range(2):\n",
    "        ftxt[i][j]=float(ftxt[i][j])\n",
    "U_label=Variable(torch.FloatTensor(np.array(ftxt).T))\n",
    "\n",
    "d=f['keypoints'][:][0]\n",
    "s_mean=Variable(torch.FloatTensor(f['mean_shape'][:]))   #159645*1;\n",
    "s=Variable(torch.FloatTensor(f['pca_basis'][:]))    #159645*50;\n",
    "P=Variable(torch.FloatTensor(np.array([[1,0,0],[0,1,0]])))\n",
    "sigma=Variable(torch.FloatTensor(f['sigma'][:]))   #50*1;\n",
    "\n",
    "alpha=Variable(torch.rand((50,1)),requires_grad=True)\n",
    "S=s_mean+s.mm(alpha)\n",
    "S=S.view(53215,3)\n",
    "f=Variable(torch.ones((2,3)),requires_grad=True)\n",
    "R=Variable(torch.rand((3,3)),requires_grad=True)\n",
    "t=Variable(torch.rand((2)),requires_grad=True)\n",
    "S_final=Variable(torch.rand(3,68))\n",
    "for i in range(68):\n",
    "    S_final[:,i]=S[d[i]]\n",
    "U_proj=(f*P).mm(R).mm(S_final)\n",
    "for i in range(68):\n",
    "    U_proj[:,i]=U_proj[:,i]+t\n",
    "loss2=torch.sum((alpha*alpha)/(sigma*sigma))\n",
    "loss1=torch.sqrt(torch.sum((U_proj-U_label)*(U_proj-U_label)))\n",
    "loss=loss1+loss2\n",
    "\n",
    "optimizer = optim.SGD([f,R,t,alpha], lr = 0.0001)\n",
    "loss_100=0\n",
    "loss_print=[]\n",
    "loss_new=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss_old=torch.sum((alpha*alpha)/(sigma*sigma))+torch.sqrt(torch.sum((U_proj-U_label)*(U_proj-U_label)))\n",
    "optimizer.zero_grad() # zero the gradient buffers\n",
    "loss.backward(retain_variables=True)\n",
    "optimizer.step() # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fry for why there is no gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Variable containing:\n",
      " 0.2840  0.7693\n",
      " 0.7379  0.2060\n",
      "[torch.FloatTensor of size 2x2]\n",
      ", Variable containing:\n",
      " 0.0928  0.4751\n",
      " 0.9091  0.1095\n",
      "[torch.FloatTensor of size 2x2]\n",
      ", Variable containing:\n",
      " 0.4022\n",
      "[torch.FloatTensor of size 1]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.rand((2,2)), requires_grad=True)\n",
    "y = Variable(torch.rand((2,2)), requires_grad=True)\n",
    "ly=torch.norm(x-y)\n",
    "optimizer = optim.SGD([x,y], lr = 0.05)\n",
    "print (x,y,ly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Variable containing:\n",
      " 1  2\n",
      " 3  4\n",
      "[torch.FloatTensor of size 2x2]\n",
      ", Variable containing:\n",
      " 8  7\n",
      " 6  5\n",
      "[torch.FloatTensor of size 2x2]\n",
      ", Variable containing:\n",
      " 49  25\n",
      "  9   1\n",
      "[torch.FloatTensor of size 2x2]\n",
      ", Variable containing:\n",
      " 21\n",
      "[torch.FloatTensor of size 1]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.Tensor([[1,2],[3,4]]), requires_grad=True)\n",
    "y = Variable(torch.Tensor([[8,7],[6,5]]), requires_grad=True)\n",
    "z=x-y\n",
    "z1=z*z\n",
    "ly=z1.mean()\n",
    "optimizer = optim.SGD([x,y], lr = 0.05)\n",
    "print (x,y,z1,ly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Variable containing:\n",
      "-3.5000 -2.5000\n",
      "-1.5000 -0.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      ", Variable containing:\n",
      " 3.5000  2.5000\n",
      " 1.5000  0.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      ", Variable containing:\n",
      " 1.1750  2.1250\n",
      " 3.0750  4.0250\n",
      "[torch.FloatTensor of size 2x2]\n",
      ", Variable containing:\n",
      " 7.8250  6.8750\n",
      " 5.9250  4.9750\n",
      "[torch.FloatTensor of size 2x2]\n",
      ", Variable containing:\n",
      " 21\n",
      "[torch.FloatTensor of size 1]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ly.backward()\n",
    "optimizer.step()\n",
    "print(x.grad,y.grad,x,y,ly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Variable containing:\n",
      " 1.1750  2.1250\n",
      " 3.0750  4.0250\n",
      "[torch.FloatTensor of size 2x2]\n",
      ", Variable containing:\n",
      " 7.8250  6.8750\n",
      " 5.9250  4.9750\n",
      "[torch.FloatTensor of size 2x2]\n",
      ", Variable containing:\n",
      " 18.9525\n",
      "[torch.FloatTensor of size 1]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "x=x\n",
    "y=y\n",
    "z=x-y\n",
    "z1=z*z\n",
    "ly=z1.mean()\n",
    "print(x,y,ly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph second time, but the buffers have already been freed. Please specify retain_variables=True when calling backward for the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-f965379d5acc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ly/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ly/anaconda2/lib/python2.7/site-packages/torch/autograd/_functions/basic_ops.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_output)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph second time, but the buffers have already been freed. Please specify retain_variables=True when calling backward for the first time."
     ]
    }
   ],
   "source": [
    "ly.backward()\n",
    "optimizer.step()\n",
    "print(x.grad,y.grad,x,y,ly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try for another problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=Variable(torch.rand((2,2)),requires_grad=True)\n",
    "y=Variable(torch.rand(1),requires_grad=True)\n",
    "z=Variable(torch.rand((2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        z[i,j]=x[i,j]+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=(z*z).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.7061  0.4787\n",
       " 0.5120  0.5318\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.2287\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Variable containing:\n",
      " 0.5414  0.0866\n",
      " 0.1533  0.1928\n",
      "[torch.FloatTensor of size 2x2]\n",
      ", Variable containing:\n",
      " 0.8708\n",
      "[torch.FloatTensor of size 1]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.2287\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(x))*0.5+2*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
